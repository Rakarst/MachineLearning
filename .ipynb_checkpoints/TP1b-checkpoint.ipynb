{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Decision Trees and Clustering Techniques\n",
    "\n",
    "## *Aprendizagem Computacional - MEI | Computação Neuronal e Sistemas Difusos - MIEB*\n",
    "\n",
    "### by Catarina Silva and Marco Simões\n",
    "\n",
    "_\n",
    "\n",
    "This assignment will assess the students knowledge on the following Machine Learning topics:\n",
    "- Decision Trees\n",
    "- Clustering Techniques\n",
    "\n",
    "The assignment is split into two sub-assignments: 1-a) Decision Trees (first week) and 1-b) Clustering Techniques (second week).\n",
    "\n",
    "Students should implement their solutions and answering the questions directly in the notebooks, and submit both files together in Inforestudante before the deadline: *06/10/2021*\n",
    "\n",
    "## Conditions: \n",
    "- *Groups:* two elements of the same PL class\n",
    "- *Duration:* 2 weeks\n",
    "- *Workload:* 8h per student\n",
    " \n",
    "\n",
    " ***\n",
    "## Group Identification:\n",
    "__Student Number:__ XXXXXXXXXX __Student Name:__ XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX <p>\n",
    "__Student Number:__ XXXXXXXXXX __Student Name:__ XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    " ***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - b) Clustering techniques\n",
    "\n",
    "\n",
    "Consider the folowing four datasets:\n",
    "\n",
    "- `Aggregation`: A. Gionis, H. Mannila, and P. Tsaparas, Clustering aggregation. ACM Transactions on Knowledge Discovery from Data (TKDD), 2007. 1(1): p. 1-30.\n",
    "- `Compound`: C.T. Zahn, Graph-theoretical methods for detecting and describing gestalt clusters. IEEE Transactions on Computers, 1971. 100(1): p. 68-86.\n",
    "- `R15`: C.J. Veenman, M.J.T. Reinders, and E. Backer, A maximum variance cluster algorithm. IEEE Trans. Pattern Analysis and Machine Intelligence, 2002. 24(9): p. 1273-1280.\n",
    "- `Spiral`: H. Chang and D.Y. Yeung, Robust path-based spectral clustering. Pattern Recognition, 2008. 41(1): p. 191-203.\n",
    "\n",
    "They are composed by matrixes of several rows and three columns: the first two correspond to the xy-coordinates of the point, and the thrid correspond to the index of the cluster they bellong to (ranging from `0` to `nclusters-1`). Each dataset have a different number of rows and clusters.\n",
    "\n",
    "\n",
    "***\n",
    "### Ex. 1\n",
    "\n",
    "Load the datasets into memory and extract, for each dataset, the number of true clusters in the data. You should populate de `datasets` and `nclusters` arrays so they end up with four cells, one for each dataset, with the dataframes (`shape=[nsamples, d=2]` and the number of clusters, respectively. Then, create a figure with four scatter plots showing the spatial distribution of the points in each dataset. You might find useful the following functions: `iloc`, from `pandas`; `subplots`, `scatter` and `set_title` from matplotlib's `pyplot`. The final result should be similar to the following image:\n",
    "\n",
    "![scatters](https://www.dropbox.com/s/h7xf9mlm2dwp158/cluster_scatters.png?dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expression cannot contain assignment, perhaps you meant \"==\"? (Temp/ipykernel_3228/2970824464.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Antho\\AppData\\Local\\Temp/ipykernel_3228/2970824464.py\"\u001b[1;36m, line \u001b[1;32m11\u001b[0m\n\u001b[1;33m    dataAgregation = pd.DataFrame(dataAgregation,shape=[nsamples, d=2])\u001b[0m\n\u001b[1;37m                                                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m expression cannot contain assignment, perhaps you meant \"==\"?\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "... # TODO add extra imports if needed\n",
    "colors = np.random.rand(50)\n",
    "# load data\n",
    "N_DATASETS = 4\n",
    "\n",
    "dataset_names = ['Aggregation', 'Compound', 'R15', 'Spiral']\n",
    "dataAgregation = pd.read_csv('Aggregation.txt',sep='\\s+',header=None)\n",
    "dataAgregation = pd.DataFrame(dataAgregation)\n",
    "dataCompound = pd.read_csv('Compound.txt',sep='\\s+',header=None)\n",
    "dataCompound = pd.DataFrame(dataCompound)\n",
    "dataSpiral = pd.read_csv('Spiral.txt',sep='\\s+',header=None)\n",
    "dataSpiral = pd.DataFrame(dataSpiral)\n",
    "dataR15 = pd.read_csv('R15.txt',sep='\\s+',header=None)\n",
    "dataR15 = pd.DataFrame(dataR15)\n",
    "\n",
    "datasets = [dataAgregation,dataCompound,dataR15,dataSpiral]\n",
    "nclusters = [max(dataAgregation[2]),max(dataCompound[2]),max(dataR15[2]),max(dataSpiral[2])]\n",
    "# Load datasets data and plot them into as scatter plots in a 2x2 figure of subplots\n",
    "fig, axes = plt.subplots(2,2, figsize=[10,10])\n",
    "\n",
    "i=0\n",
    "for x in range(0,2):\n",
    "    for y in range(0,2):\n",
    "        axes[x,y].set_title(dataset_names[i])\n",
    "        axes[x,y].plot(datasets[i][0],datasets[i][1],'.r')\n",
    "        i+=1\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Ex. 2\n",
    "\n",
    "Create a function `calc_dist( xi, xj, dist_type )` that, given two feature vectors (`xi` and `xj`) and the `type of distance` metric to use (`dist_type`, with possible values: `euclidean, manhattan, chebychev`), computes and returns the correspondent distance value between the two feature vectors. **NOTE:** You may not resort to third-party predefined distance functions, such as the ones provided by the `scipy.spatial.distance` module. You must compute the distance from the data, using only simple mathematical and algebric functions, such as `sum, sqrt, abs` and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dist( xi, xj, dist_type ):\n",
    "\n",
    "    # TODO Write your code here\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Ex. 3\n",
    "Create a function `gen_random_centroids( n, min_coords, max_coors )` that generates `n` random points within the coordinate limits provided in the arrays `min_coords` and `max_coords` (both of `shape = [1,d]`, with `d` being the number of coordinates of each point - in our datasets, `d=2`). The function must output a matrix of shape `[n, d]`, where each row contains the coordinates of a centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_random_centroids( n, min_coords, max_coords ):\n",
    "    # TODO Write your code here\n",
    "\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "print(gen_random_centroids(3, [2, -3], [12, 5]))\n",
    "\n",
    "# Example Output: \n",
    "# - Since the values are random, your output might/should be different. Only the shape should be the same. \n",
    "# - Notice the values are within the range limits provides by argument to the fucntion.\n",
    "\n",
    "'''\n",
    "[[10.93543569  3.45456041]\n",
    " [11.6848827   3.90977382]\n",
    " [ 5.15555478 -1.78510517]]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Ex. 4\n",
    "Create a function `label_data( data, centroids, dist_type='euclidean' )` that attributes a cluster label for each data record based on its distance to the centroids. The `data` shape is `[n_samples, d]`and the `centroids` shape is `[n, d]`. You should output an array of shape `[n_samples, 1]` with values ranging between `0` and `n-1`, corresponding to the index of the closest `centroid` for each data record. Please consider different types of distance metrics, making use of the function developed in **Ex. 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_data( data, centroids, dist_type='euclidean' ):\n",
    "    # TODO Write your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "ex_data = pd.DataFrame( np.array( [[0,0],[1,0],[2,0],[1,0],[1,1],[1,2]] ) )\n",
    "ex_centroids = np.array( [[0.2,0.2],[1.6,0.6],[5.0,5.0]] )\n",
    "ex_distance = 'euclidean'\n",
    "\n",
    "ex_labels = label_data(ex_data, ex_centroids, ex_distance)\n",
    "print( ex_labels )\n",
    "\n",
    "# expected result\n",
    "'''\n",
    "[[0]\n",
    " [0]\n",
    " [1]\n",
    " [0]\n",
    " [1]\n",
    " [1]]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Ex. 5\n",
    "\n",
    "Create a function `update_centroids( data, labels, centroids )` that updates the centroids coordinates based on the mass center of the data records associated with him. If a centroid has no record associated to him, its value must remain unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_centroids( data, labels, centroids ):\n",
    "    \n",
    "    # TODO Write your code here\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "ex_data = pd.DataFrame( np.array( [[0,0],[1,0],[2,0],[1,0],[1,1],[1,2]] ) )\n",
    "ex_labels = np.array([[0],[0],[1],[0],[1],[1]])\n",
    "ex_centroids = np.array( [[0.2,0.2],[1.6,0.6],[5.0,5.0]] )\n",
    "\n",
    "new_centroids = update_centroids(ex_data, ex_labels, ex_centroids)\n",
    "\n",
    "print( new_centroids )\n",
    "\n",
    "# expected result\n",
    "'''\n",
    "[[0.66666667 0.        ]\n",
    " [1.33333333 1.        ]\n",
    " [5.         5.        ]]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Ex. 6\n",
    "\n",
    "Create a function `plot_clusters( data, labels, centroids=None )` that plots a _scatter plot_ of the data, coloring the points of each cluster with a different color, and marking the centroids, if provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters( data, labels, centroids=None):\n",
    "    \n",
    "    # TODO Write your code here\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Ex. 7\n",
    "\n",
    "Create a function `kmeans( data, n, dist_type='euclidean' )` that, given a dataset with shape `[n_samples, d]`, clusters the data into `n` sets using the `k-means` algorithm. Make use of the functions you developed previously; you cannot resort to `sklearn` nor other implementations. The function should return the final labels, the final centroid coordinates and the number of iterations run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans( data, n, dist_type='euclidean', max_iter=20):\n",
    "    old_labels = np.empty((data.shape[0], 1))\n",
    "    new_labels = np.random.randint(n, size=(data.shape[0], 1))\n",
    "\n",
    "    # initialize randomly the centroids\n",
    "    # TODO Code Here\n",
    "\n",
    "    i = 0\n",
    "    # REPEAT UNTIL i REACHES MAX_ITERATIONS OR UNTIL NEW LABELS AND OLD LABELS ARE EQUAL \n",
    "    while ...# TODO Write your code here \n",
    "    \n",
    "        old_labels = new_labels\n",
    "\n",
    "        # get new_labels from data using the current centroid locations \n",
    "        # TODO Write your code here \n",
    "       \n",
    "        # update the centroid locations based on the new labels \n",
    "        # TODO Write your code here \n",
    "              \n",
    "        i += 1\n",
    "    \n",
    "    return new_labels, centroids, i \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Ex. 8\n",
    "\n",
    "The code bellow generates 3 figures with with the final result of applying the `kmeans` function to the data using the three different types of distance (`euclidean`, `manhattan` and `chebychev`). Did the distance type influence the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dt in ['euclidean', 'manhattan', 'chebychev']:\n",
    "\n",
    "    fig, axes = plt.subplots(2,2, figsize=[10,10])\n",
    "    fig.suptitle('Distance type: %s' %dt)\n",
    "\n",
    "    axs = axes.flatten()\n",
    "\n",
    "    for ds in range(len(datasets)):\n",
    "        labels, centroids, it = kmeans(datasets[ds], nclusters[ds], dt)\n",
    "        axs[ds].scatter(datasets[ds].iloc[:,0],datasets[ds].iloc[:,1], c=labels)\n",
    "        axs[ds].scatter(centroids[:,0],centroids[:,1], marker='x', color='r')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Answer:**\n",
    "\n",
    "`TODO write answer here ...`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Ex. 9\n",
    "\n",
    "Run the `kmeans` function 10 times with each distance type (in each dataset). Compute report the average and standard deviation of the number of iterations taken by the algorithm. Comment your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Write your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Answer:**\n",
    "\n",
    "`TODO write answer here ...`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Ex 10\n",
    "\n",
    "Use the `DBSCAN` method from the `sklearn.cluster` module to cluster de data. Try different values of `eps` and `min_samples`. Visualize the results making use of the function developed `plot_clusters` developed in **Ex. 6**. Discuss the following points:\n",
    "1. In what way does the `eps` value influence the result of the algorithm? Does the same value work well for all datasets or it should be tuned for each dataset?\n",
    "2. In what way does the `min_samples` value influence the result of the algorithm? Does the same value work well for all datasets or it should be tuned for each dataset?\n",
    "3. Comment de differences in the results obtained by `DBSCAN` and `kmeans`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Answers:**\n",
    "\n",
    "1. \n",
    "`TODO write answers here ...`\n",
    "\n",
    "2. \n",
    "`TODO write answers here ...`\n",
    "\n",
    "3. \n",
    "`TODO write answers here ...`"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
